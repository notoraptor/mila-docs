<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Jax &mdash; MILA Technical Documentation latest documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=c6e86fd7"></script>
        <script src="../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../../../_static/copybutton.js?v=f281be69"></script>
        <script src="../../../_static/documentation_options.js?v=c6e86fd7"></script>
        <script src="../../../_static/documentation_options_fix.js?v=1c8886ec"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Distributed Training" href="../../distributed/index.html" />
    <link rel="prev" title="Jax Setup" href="../jax_setup/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html">
            
              <img src="../../../_static/image.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                latest
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Purpose.html">Purpose of this documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Purpose.html#contributing">Contributing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">How-tos and Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Userguide.html">User’s guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Handbook.html">AI tooling and methodology handbook</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Systems and services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Information.html">Computing infrastructure and policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Extra_compute.html">Computational resources outside of Mila</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">General theory</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Theory_cluster.html">What is a computer cluster?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Theory_cluster.html#parts-of-a-computing-cluster">Parts of a computing cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Theory_cluster.html#unix">UNIX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Theory_cluster.html#the-workload-manager">The workload manager</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Theory_cluster.html#processing-data">Processing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Theory_cluster.html#software-on-the-cluster">Software on the cluster</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Minimal Examples</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Software Frameworks</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../pytorch_setup/index.html">PyTorch Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax_setup/index.html">Jax Setup</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Jax</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed/index.html">Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../good_practices/index.html">Good practices</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Extras</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Acknowledgement.html">Acknowledging Mila</a></li>
<li class="toctree-l1"><a class="reference external" href="https://datasets.server.mila.quebec/">Mila Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Audio_video.html">Audio and video resources at Mila</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../VSCode.html">Visual Studio Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../IDT.html">Who, what, where is IDT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Cheatsheet.html">Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Chatbot.html">Chatbot for Mila</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Environmental_impact.html">Environmental Impact</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MILA Technical Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Software Frameworks</a></li>
      <li class="breadcrumb-item active">Jax</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/mila-iqia/mila-docs/blob/master/docs/examples/frameworks/jax/index.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="jax">
<span id="id1"></span><h1>Jax<a class="headerlink" href="#jax" title="Link to this heading"></a></h1>
<p><strong>Prerequisites</strong>
Make sure to read the following sections of the documentation before using this
example:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../jax_setup/index.html"><span class="doc">Jax Setup</span></a></p></li>
<li><p><a class="reference internal" href="../../distributed/single_gpu/index.html"><span class="doc">Single GPU Job</span></a></p></li>
</ul>
<p>The full source code for this example is available on <a class="reference external" href="https://github.com/mila-iqia/mila-docs/tree/master/docs/examples/frameworks/jax">the mila-docs GitHub
repository.</a></p>
<p><strong>job.sh</strong></p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span># distributed/single_gpu/job.sh -&gt; frameworks/jax/job.sh
<span class="w"> </span>#!/bin/bash
<span class="w"> </span>#SBATCH --gpus-per-task=rtx8000:1
<span class="w"> </span>#SBATCH --cpus-per-task=4
<span class="w"> </span>#SBATCH --ntasks-per-node=1
<span class="w"> </span>#SBATCH --mem=16G
<span class="w"> </span>#SBATCH --time=00:15:00
<span class="w"> </span>
<span class="w"> </span>
<span class="w"> </span># Echo time and hostname into log
<span class="w"> </span>echo &quot;Date:     $(date)&quot;
<span class="w"> </span>echo &quot;Hostname: $(hostname)&quot;
<span class="w"> </span>
<span class="w"> </span>
<span class="w"> </span># Ensure only anaconda/3 module loaded.
<span class="w"> </span>module --quiet purge
<span class="w"> </span># This example uses Conda to manage package dependencies.
<span class="w"> </span># See https://docs.mila.quebec/Userguide.html#conda for more information.
<span class="w"> </span>module load anaconda/3
<span class="gd">-module load cuda/11.7</span>
<span class="w"> </span>
<span class="w"> </span># Creating the environment for the first time:
<span class="gd">-# conda create -y -n pytorch python=3.9 pytorch torchvision torchaudio \</span>
<span class="gd">-#     pytorch-cuda=11.7 -c pytorch -c nvidia</span>
<span class="gd">-# Other conda packages:</span>
<span class="gd">-# conda install -y -n pytorch -c conda-forge rich tqdm</span>
<span class="gi">+# conda create -y -n jax_ex -c &quot;nvidia/label/cuda-11.8.0&quot; cuda python=3.9 virtualenv pip</span>
<span class="gi">+# conda activate jax_ex</span>
<span class="gi">+# Install Jax using `pip`</span>
<span class="gi">+# *Please note* that as soon as you install packages from `pip install`, you</span>
<span class="gi">+# should not install any more packages using `conda install`</span>
<span class="gi">+# pip install --upgrade &quot;jax[cuda11_pip]&quot; \</span>
<span class="gi">+#    -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html</span>
<span class="gi">+# Other pip packages:</span>
<span class="gi">+# pip install pillow optax rich torch torchvision flax tqdm</span>
<span class="w"> </span>
<span class="gd">-# Activate pre-existing environment.</span>
<span class="gd">-conda activate pytorch</span>
<span class="gi">+# Activate the environment:</span>
<span class="gi">+conda activate jax_ex</span>
<span class="w"> </span>
<span class="w"> </span>
<span class="w"> </span># Stage dataset into $SLURM_TMPDIR
<span class="w"> </span>mkdir -p $SLURM_TMPDIR/data
<span class="w"> </span>cp /network/datasets/cifar10/cifar-10-python.tar.gz $SLURM_TMPDIR/data/
<span class="w"> </span># General-purpose alternatives combining copy and unpack:
<span class="w"> </span>#     unzip   /network/datasets/some/file.zip -d $SLURM_TMPDIR/data/
<span class="w"> </span>#     tar -xf /network/datasets/some/file.tar -C $SLURM_TMPDIR/data/
<span class="w"> </span>
<span class="w"> </span>
<span class="gd">-# Fixes issues with MIG-ed GPUs with versions of PyTorch &lt; 2.0</span>
<span class="gd">-unset CUDA_VISIBLE_DEVICES</span>
<span class="gd">-</span>
<span class="w"> </span># Execute Python script
<span class="w"> </span>python main.py
</pre></div>
</div>
<p><strong>main.py</strong></p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span><span class="w"> </span># distributed/single_gpu/main.py -&gt; frameworks/jax/main.py
<span class="gd">-&quot;&quot;&quot;Single-GPU training example.&quot;&quot;&quot;</span>
<span class="gi">+&quot;&quot;&quot;Single-GPU training example.</span>
<span class="gi">+</span>
<span class="gi">+This Jax example is heavily based on the following examples:</span>
<span class="gi">+</span>
<span class="gi">+* https://juliusruseckas.github.io/ml/flax-cifar10.html</span>
<span class="gi">+* https://github.com/fattorib/Flax-ResNets/blob/master/main_flax.py</span>
<span class="gi">+&quot;&quot;&quot;</span>
<span class="w"> </span>import argparse
<span class="w"> </span>import logging
<span class="gi">+import math</span>
<span class="w"> </span>import os
<span class="w"> </span>from pathlib import Path
<span class="gi">+from typing import Any, Sequence</span>
<span class="w"> </span>
<span class="gi">+import PIL.Image</span>
<span class="gi">+import jax</span>
<span class="gi">+import jax.numpy as jnp</span>
<span class="gi">+import numpy as np</span>
<span class="gi">+import optax</span>
<span class="w"> </span>import rich.logging
<span class="w"> </span>import torch
<span class="gd">-from torch import Tensor, nn</span>
<span class="gd">-from torch.nn import functional as F</span>
<span class="gi">+</span>
<span class="gi">+from flax.training import train_state, common_utils</span>
<span class="w"> </span>from torch.utils.data import DataLoader, random_split
<span class="gd">-from torchvision import transforms</span>
<span class="w"> </span>from torchvision.datasets import CIFAR10
<span class="gd">-from torchvision.models import resnet18</span>
<span class="w"> </span>from tqdm import tqdm
<span class="w"> </span>
<span class="gi">+from model import ResNet</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class TrainState(train_state.TrainState):</span>
<span class="gi">+    batch_stats: Any</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+class ToArray(torch.nn.Module):</span>
<span class="gi">+    &quot;&quot;&quot;convert image to float and 0-1 range&quot;&quot;&quot;</span>
<span class="gi">+    dtype = np.float32</span>
<span class="gi">+</span>
<span class="gi">+    def __call__(self, x):</span>
<span class="gi">+        assert isinstance(x, PIL.Image.Image)</span>
<span class="gi">+        x = np.asarray(x, dtype=self.dtype)</span>
<span class="gi">+        x /= 255.0</span>
<span class="gi">+        return x</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+def numpy_collate(batch: Sequence):</span>
<span class="gi">+    if isinstance(batch[0], np.ndarray):</span>
<span class="gi">+        return np.stack(batch)</span>
<span class="gi">+    elif isinstance(batch[0], (tuple, list)):</span>
<span class="gi">+        transposed = zip(*batch)</span>
<span class="gi">+        return [numpy_collate(samples) for samples in transposed]</span>
<span class="gi">+    else:</span>
<span class="gi">+        return np.array(batch)</span>
<span class="gi">+</span>
<span class="w"> </span>
<span class="w"> </span>def main():
<span class="w"> </span>    # Use an argument parser so we can pass hyperparameters from the command line.
<span class="w"> </span>    parser = argparse.ArgumentParser(description=__doc__)
<span class="w"> </span>    parser.add_argument(&quot;--epochs&quot;, type=int, default=10)
<span class="w"> </span>    parser.add_argument(&quot;--learning-rate&quot;, type=float, default=5e-4)
<span class="w"> </span>    parser.add_argument(&quot;--weight-decay&quot;, type=float, default=1e-4)
<span class="w"> </span>    parser.add_argument(&quot;--batch-size&quot;, type=int, default=128)
<span class="w"> </span>    args = parser.parse_args()
<span class="w"> </span>
<span class="w"> </span>    epochs: int = args.epochs
<span class="w"> </span>    learning_rate: float = args.learning_rate
<span class="w"> </span>    weight_decay: float = args.weight_decay
<span class="gi">+    # NOTE: This is the &quot;local&quot; batch size, per-GPU.</span>
<span class="w"> </span>    batch_size: int = args.batch_size
<span class="w"> </span>
<span class="w"> </span>    # Check that the GPU is available
<span class="w"> </span>    assert torch.cuda.is_available() and torch.cuda.device_count() &gt; 0
<span class="gd">-    device = torch.device(&quot;cuda&quot;, 0)</span>
<span class="gi">+    rng = jax.random.PRNGKey(0)</span>
<span class="w"> </span>
<span class="w"> </span>    # Setup logging (optional, but much better than using print statements)
<span class="w"> </span>    logging.basicConfig(
<span class="w"> </span>        level=logging.INFO,
<span class="w"> </span>        handlers=[rich.logging.RichHandler(markup=True)],  # Very pretty, uses the `rich` package.
<span class="w"> </span>    )
<span class="w"> </span>
<span class="w"> </span>    logger = logging.getLogger(__name__)
<span class="w"> </span>
<span class="gd">-    # Create a model and move it to the GPU.</span>
<span class="gd">-    model = resnet18(num_classes=10)</span>
<span class="gd">-    model.to(device=device)</span>
<span class="gi">+    # Create a model.</span>
<span class="gi">+    model = ResNet(</span>
<span class="gi">+        10,</span>
<span class="gi">+        channel_list = [64, 128, 256, 512],</span>
<span class="gi">+        num_blocks_list = [2, 2, 2, 2],</span>
<span class="gi">+        strides = [1, 1, 2, 2, 2],</span>
<span class="gi">+        head_p_drop = 0.3</span>
<span class="gi">+    )</span>
<span class="w"> </span>
<span class="gd">-    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)</span>
<span class="gi">+    @jax.jit</span>
<span class="gi">+    def initialize(params_rng, image_size=32):</span>
<span class="gi">+        init_rngs = {&#39;params&#39;: params_rng}</span>
<span class="gi">+        input_shape = (1, image_size, image_size, 3)</span>
<span class="gi">+        variables = model.init(init_rngs, jnp.ones(input_shape, jnp.float32), train=False)</span>
<span class="gi">+        return variables</span>
<span class="w"> </span>
<span class="w"> </span>    # Setup CIFAR10
<span class="w"> </span>    num_workers = get_num_workers()
<span class="w"> </span>    dataset_path = Path(os.environ.get(&quot;SLURM_TMPDIR&quot;, &quot;.&quot;)) / &quot;data&quot;
<span class="w"> </span>    train_dataset, valid_dataset, test_dataset = make_datasets(str(dataset_path))
<span class="w"> </span>    train_dataloader = DataLoader(
<span class="w"> </span>        train_dataset,
<span class="w"> </span>        batch_size=batch_size,
<span class="w"> </span>        num_workers=num_workers,
<span class="w"> </span>        shuffle=True,
<span class="gi">+        collate_fn=numpy_collate,</span>
<span class="w"> </span>    )
<span class="w"> </span>    valid_dataloader = DataLoader(
<span class="w"> </span>        valid_dataset,
<span class="w"> </span>        batch_size=batch_size,
<span class="w"> </span>        num_workers=num_workers,
<span class="w"> </span>        shuffle=False,
<span class="gi">+        collate_fn=numpy_collate,</span>
<span class="w"> </span>    )
<span class="w"> </span>    test_dataloader = DataLoader(  # NOTE: Not used in this example.
<span class="w"> </span>        test_dataset,
<span class="w"> </span>        batch_size=batch_size,
<span class="w"> </span>        num_workers=num_workers,
<span class="w"> </span>        shuffle=False,
<span class="gi">+        collate_fn=numpy_collate,</span>
<span class="gi">+    )</span>
<span class="gi">+</span>
<span class="gi">+    train_steps_per_epoch = math.ceil(len(train_dataset) / batch_size)</span>
<span class="gi">+    num_train_steps = train_steps_per_epoch * epochs</span>
<span class="gi">+    shedule_fn = optax.cosine_onecycle_schedule(transition_steps=num_train_steps, peak_value=learning_rate)</span>
<span class="gi">+    optimizer = optax.adamw(learning_rate=shedule_fn, weight_decay=weight_decay)</span>
<span class="gi">+</span>
<span class="gi">+    params_rng, dropout_rng = jax.random.split(rng)</span>
<span class="gi">+    variables = initialize(params_rng)</span>
<span class="gi">+</span>
<span class="gi">+    state = TrainState.create(</span>
<span class="gi">+        apply_fn = model.apply,</span>
<span class="gi">+        params = variables[&#39;params&#39;],</span>
<span class="gi">+        batch_stats = variables[&#39;batch_stats&#39;],</span>
<span class="gi">+        tx = optimizer</span>
<span class="w"> </span>    )
<span class="w"> </span>
<span class="w"> </span>    # Checkout the &quot;checkpointing and preemption&quot; example for more info!
<span class="w"> </span>    logger.debug(&quot;Starting training from scratch.&quot;)
<span class="w"> </span>
<span class="w"> </span>    for epoch in range(epochs):
<span class="w"> </span>        logger.debug(f&quot;Starting epoch {epoch}/{epochs}&quot;)
<span class="w"> </span>
<span class="gd">-        # Set the model in training mode (important for e.g. BatchNorm and Dropout layers)</span>
<span class="gd">-        model.train()</span>
<span class="gd">-</span>
<span class="w"> </span>        # NOTE: using a progress bar from tqdm because it&#39;s nicer than using `print`.
<span class="w"> </span>        progress_bar = tqdm(
<span class="w"> </span>            total=len(train_dataloader),
<span class="w"> </span>            desc=f&quot;Train epoch {epoch}&quot;,
<span class="w"> </span>        )
<span class="w"> </span>
<span class="w"> </span>        # Training loop
<span class="gd">-        for batch in train_dataloader:</span>
<span class="gd">-            # Move the batch to the GPU before we pass it to the model</span>
<span class="gd">-            batch = tuple(item.to(device) for item in batch)</span>
<span class="gd">-            x, y = batch</span>
<span class="gd">-</span>
<span class="gd">-            # Forward pass</span>
<span class="gd">-            logits: Tensor = model(x)</span>
<span class="gd">-</span>
<span class="gd">-            loss = F.cross_entropy(logits, y)</span>
<span class="gi">+        for input, target in train_dataloader:</span>
<span class="gi">+            batch = {</span>
<span class="gi">+                &#39;image&#39;: input,</span>
<span class="gi">+                &#39;label&#39;: target,</span>
<span class="gi">+            }</span>
<span class="gi">+            state, loss, accuracy = train_step(state, batch, dropout_rng)</span>
<span class="w"> </span>
<span class="gd">-            optimizer.zero_grad()</span>
<span class="gd">-            loss.backward()</span>
<span class="gd">-            optimizer.step()</span>
<span class="gi">+            logger.debug(f&quot;Accuracy: {accuracy:.2%}&quot;)</span>
<span class="gi">+            logger.debug(f&quot;Average Loss: {loss}&quot;)</span>
<span class="w"> </span>
<span class="gd">-            # Calculate some metrics:</span>
<span class="gd">-            n_correct_predictions = logits.detach().argmax(-1).eq(y).sum()</span>
<span class="gd">-            n_samples = y.shape[0]</span>
<span class="gd">-            accuracy = n_correct_predictions / n_samples</span>
<span class="gd">-</span>
<span class="gd">-            logger.debug(f&quot;Accuracy: {accuracy.item():.2%}&quot;)</span>
<span class="gd">-            logger.debug(f&quot;Average Loss: {loss.item()}&quot;)</span>
<span class="gd">-</span>
<span class="gd">-            # Advance the progress bar one step and update the progress bar text.</span>
<span class="gi">+            # Advance the progress bar one step, and update the &quot;postfix&quot; () the progress bar. (nicer than just)</span>
<span class="w"> </span>            progress_bar.update(1)
<span class="gd">-            progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy.item())</span>
<span class="gi">+            progress_bar.set_postfix(loss=loss, accuracy=accuracy)</span>
<span class="w"> </span>        progress_bar.close()
<span class="w"> </span>
<span class="gd">-        val_loss, val_accuracy = validation_loop(model, valid_dataloader, device)</span>
<span class="gi">+        val_loss, val_accuracy = validation_loop(state, valid_dataloader)</span>
<span class="w"> </span>        logger.info(f&quot;Epoch {epoch}: Val loss: {val_loss:.3f} accuracy: {val_accuracy:.2%}&quot;)
<span class="w"> </span>
<span class="w"> </span>    print(&quot;Done!&quot;)
<span class="w"> </span>
<span class="w"> </span>
<span class="gd">-@torch.no_grad()</span>
<span class="gd">-def validation_loop(model: nn.Module, dataloader: DataLoader, device: torch.device):</span>
<span class="gd">-    model.eval()</span>
<span class="gi">+def cross_entropy_loss(logits, labels, num_classes=10):</span>
<span class="gi">+    one_hot_labels = common_utils.onehot(labels, num_classes=num_classes)</span>
<span class="gi">+    loss = optax.softmax_cross_entropy(logits=logits, labels=one_hot_labels)</span>
<span class="gi">+    loss = jnp.mean(loss)</span>
<span class="gi">+    return loss</span>
<span class="w"> </span>
<span class="gd">-    total_loss = 0.0</span>
<span class="gd">-    n_samples = 0</span>
<span class="gd">-    correct_predictions = 0</span>
<span class="w"> </span>
<span class="gd">-    for batch in dataloader:</span>
<span class="gd">-        batch = tuple(item.to(device) for item in batch)</span>
<span class="gd">-        x, y = batch</span>
<span class="gi">+@jax.jit</span>
<span class="gi">+def train_step(state, batch, dropout_rng):</span>
<span class="gi">+    dropout_rng = jax.random.fold_in(dropout_rng, state.step)</span>
<span class="w"> </span>
<span class="gd">-        logits: Tensor = model(x)</span>
<span class="gd">-        loss = F.cross_entropy(logits, y)</span>
<span class="gi">+    def loss_fn(params):</span>
<span class="gi">+        variables = {&#39;params&#39;: params, &#39;batch_stats&#39;: state.batch_stats}</span>
<span class="gi">+        logits, new_model_state = state.apply_fn(variables, batch[&#39;image&#39;], train=True,</span>
<span class="gi">+                                                 rngs={&#39;dropout&#39;: dropout_rng}, mutable=&#39;batch_stats&#39;)</span>
<span class="gi">+        loss = cross_entropy_loss(logits, batch[&#39;label&#39;])</span>
<span class="gi">+        accuracy = jnp.sum(jnp.argmax(logits, -1) == batch[&#39;label&#39;])</span>
<span class="gi">+        return loss, (accuracy, new_model_state)</span>
<span class="w"> </span>
<span class="gd">-        batch_n_samples = x.shape[0]</span>
<span class="gd">-        batch_correct_predictions = logits.argmax(-1).eq(y).sum()</span>
<span class="gi">+    (loss, (accuracy, new_model_state)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)</span>
<span class="gi">+    new_state = state.apply_gradients(grads=grads, batch_stats=new_model_state[&#39;batch_stats&#39;])</span>
<span class="gi">+    return new_state, loss, accuracy</span>
<span class="w"> </span>
<span class="gd">-        total_loss += loss.item()</span>
<span class="gd">-        n_samples += batch_n_samples</span>
<span class="gd">-        correct_predictions += batch_correct_predictions</span>
<span class="w"> </span>
<span class="gd">-    accuracy = correct_predictions / n_samples</span>
<span class="gi">+@jax.jit</span>
<span class="gi">+def validation_step(state, batch):</span>
<span class="gi">+    variables = {&#39;params&#39;: state.params, &#39;batch_stats&#39;: state.batch_stats}</span>
<span class="gi">+    logits = state.apply_fn(variables, batch[&#39;image&#39;], train=False, mutable=False)</span>
<span class="gi">+    loss = cross_entropy_loss(logits, batch[&#39;label&#39;])</span>
<span class="gi">+    batch_correct_predictions = jnp.sum(jnp.argmax(logits, -1) == batch[&#39;label&#39;])</span>
<span class="gi">+    return loss, batch_correct_predictions</span>
<span class="gi">+</span>
<span class="gi">+</span>
<span class="gi">+@torch.no_grad()</span>
<span class="gi">+def validation_loop(state, dataloader: DataLoader):</span>
<span class="gi">+    losses = []</span>
<span class="gi">+    correct_predictions = []</span>
<span class="gi">+    for input, target in dataloader:</span>
<span class="gi">+        batch = {</span>
<span class="gi">+            &#39;image&#39;: input,</span>
<span class="gi">+            &#39;label&#39;: target,</span>
<span class="gi">+        }</span>
<span class="gi">+        loss, batch_correct_predictions = validation_step(state, batch)</span>
<span class="gi">+        losses.append(loss)</span>
<span class="gi">+        correct_predictions.append(batch_correct_predictions)</span>
<span class="gi">+</span>
<span class="gi">+    total_loss = np.sum(losses)</span>
<span class="gi">+    accuracy = np.mean(correct_predictions)</span>
<span class="w"> </span>    return total_loss, accuracy
<span class="w"> </span>
<span class="w"> </span>
<span class="w"> </span>def make_datasets(
<span class="w"> </span>    dataset_path: str,
<span class="w"> </span>    val_split: float = 0.1,
<span class="w"> </span>    val_split_seed: int = 42,
<span class="w"> </span>):
<span class="w"> </span>    &quot;&quot;&quot;Returns the training, validation, and test splits for CIFAR10.
<span class="w"> </span>
<span class="w"> </span>    NOTE: We don&#39;t use image transforms here for simplicity.
<span class="w"> </span>    Having different transformations for train and validation would complicate things a bit.
<span class="w"> </span>    Later examples will show how to do the train/val/test split properly when using transforms.
<span class="w"> </span>    &quot;&quot;&quot;
<span class="w"> </span>    train_dataset = CIFAR10(
<span class="gd">-        root=dataset_path, transform=transforms.ToTensor(), download=True, train=True</span>
<span class="gi">+        root=dataset_path, transform=ToArray(), download=True, train=True</span>
<span class="w"> </span>    )
<span class="w"> </span>    test_dataset = CIFAR10(
<span class="gd">-        root=dataset_path, transform=transforms.ToTensor(), download=True, train=False</span>
<span class="gi">+        root=dataset_path, transform=ToArray(), download=True, train=False</span>
<span class="w"> </span>    )
<span class="w"> </span>    # Split the training dataset into a training and validation set.
<span class="w"> </span>    n_samples = len(train_dataset)
<span class="w"> </span>    n_valid = int(val_split * n_samples)
<span class="w"> </span>    n_train = n_samples - n_valid
<span class="w"> </span>    train_dataset, valid_dataset = random_split(
<span class="w"> </span>        train_dataset, (n_train, n_valid), torch.Generator().manual_seed(val_split_seed)
<span class="w"> </span>    )
<span class="w"> </span>    return train_dataset, valid_dataset, test_dataset
<span class="w"> </span>
<span class="w"> </span>
<span class="w"> </span>def get_num_workers() -&gt; int:
<span class="w"> </span>    &quot;&quot;&quot;Gets the optimal number of DatLoader workers to use in the current job.&quot;&quot;&quot;
<span class="w"> </span>    if &quot;SLURM_CPUS_PER_TASK&quot; in os.environ:
<span class="w"> </span>        return int(os.environ[&quot;SLURM_CPUS_PER_TASK&quot;])
<span class="w"> </span>    if hasattr(os, &quot;sched_getaffinity&quot;):
<span class="w"> </span>        return len(os.sched_getaffinity(0))
<span class="w"> </span>    return torch.multiprocessing.cpu_count()
<span class="w"> </span>
<span class="w"> </span>
<span class="w"> </span>if __name__ == &quot;__main__&quot;:
<span class="w"> </span>    main()
</pre></div>
</div>
<p><strong>model.py</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Sequence</span>

<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>

<span class="kn">from</span> <span class="nn">flax</span> <span class="kn">import</span> <span class="n">linen</span> <span class="k">as</span> <span class="n">nn</span>


<span class="n">ModuleDef</span> <span class="o">=</span> <span class="n">Any</span>


<span class="k">class</span> <span class="nc">ConvBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">channels</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">norm</span><span class="p">:</span> <span class="n">ModuleDef</span>
    <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">act</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">channels</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                    <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">kernel_init</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">kaiming_normal</span><span class="p">())(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">swish</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">ResidualBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">channels</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">conv_block</span><span class="p">:</span> <span class="n">ModuleDef</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">channels</span>
        <span class="n">conv_block</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_block</span>

        <span class="n">shortcut</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">residual</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">residual</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">shortcut</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">residual</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="n">shortcut</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">shortcut</span><span class="p">)</span>

        <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param</span><span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">zeros</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">shortcut</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">residual</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">swish</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="k">class</span> <span class="nc">Stage</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">channels</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">block</span><span class="p">:</span> <span class="n">ModuleDef</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_blocks</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">channels</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">Body</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">channel_list</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">num_blocks_list</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">strides</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">stage</span><span class="p">:</span> <span class="n">ModuleDef</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">channels</span><span class="p">,</span> <span class="n">num_blocks</span><span class="p">,</span> <span class="n">stride</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">channel_list</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_blocks_list</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">num_blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">Stem</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">channel_list</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">conv_block</span><span class="p">:</span> <span class="n">ModuleDef</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
        <span class="k">for</span> <span class="n">channels</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">channel_list</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_block</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">Head</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">classes</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="n">ModuleDef</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">classes</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">channel_list</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">num_blocks_list</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">strides</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">head_p_drop</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span> <span class="n">use_running_average</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head_p_drop</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="ow">not</span> <span class="n">train</span><span class="p">)</span>
        <span class="n">conv_block</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ConvBlock</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
        <span class="n">residual_block</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">,</span> <span class="n">conv_block</span><span class="o">=</span><span class="n">conv_block</span><span class="p">)</span>
        <span class="n">stage</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">Stage</span><span class="p">,</span> <span class="n">block</span><span class="o">=</span><span class="n">residual_block</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">Stem</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">conv_block</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Body</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">channel_list</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_blocks_list</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">stage</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Head</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p><strong>Running this example</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>sbatch<span class="w"> </span>job.sh
</pre></div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../jax_setup/index.html" class="btn btn-neutral float-left" title="Jax Setup" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../distributed/index.html" class="btn btn-neutral float-right" title="Distributed Training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
<script type="text/javascript">
  window.onload = function() {
      $(".toggle > *").hide();
      $(".toggle .header").show();
      $(".toggle .header").click(function() {
          $(this).parent().children().not(".header").toggle(400);
          $(this).parent().children(".header").toggleClass("open");
      })
  };
</script>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>